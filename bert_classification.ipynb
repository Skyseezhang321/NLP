{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-2我们知道，在BERT的输入中，cls是一个，sep是大于等于一个。\n",
    "\n",
    "'[CLS]'必须出现在样本段落的开头，一个段落可以有一句话也可以有多句话，每句话的结尾必须是'[SEP]'。\n",
    "\n",
    "例如：['[CLS]', 'this', 'is', 'blue', '[SEP]', 'that', 'is', 'red', '[SEP]']\n",
    "\n",
    "我们需要对输入BERT的数据进行处理,例如：\n",
    "\n",
    "words = [self.CLS_TOKEN] + words + [self.SEP_TOKEN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Tokenizer\n",
    "调用tokenizer，使用tokenizer分割输入，将tokens转为ids。如下：\n",
    "\n",
    "self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-chinese') \n",
    "\n",
    "words = self.bert_tokenizer.tokenize(''.join(words)) \n",
    "\n",
    "feature = self.bert_tokenizer.convert_tokens_to_ids(sent + [self.PAD_TOKEN for _ in range(max_sent_len - len(sent))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果觉得BERT给的token不够用，或希望在BERT给的token中加入自己的token，加入以下代码即可：\n",
    "\n",
    "例如，想要加入大写字母：\n",
    "\n",
    "self.bert_tokenizer.add_tokens([chr(i) for i in range(ord(\"A\"), ord(\"Z\") + 1)]) \n",
    "\n",
    "args.len_token = len(self.bert_tokenizer)\n",
    "\n",
    "此处使用了len_token记录self.bert_tokenizer新的token大小，因为要对模型进行更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.模型使用\n",
    "调用BertModel，因为改变了tokenizer所以对模型的token参数进行更新，然后就可以正常使用BERT-Model啦！\n",
    "\n",
    "self.BertModel = BertModel.from_pretrained('bert-base-chinese') \n",
    "#加入了A-Z，重新resize一下大小\n",
    "\n",
    "self.BertModel.resize_token_embeddings(self.args.len_token)  \n",
    "outputs = self.BertModel(input_ids=ii, token_type_ids=tti, attention_mask=am)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文使用了预训练模型bert-base-chinese做例子，其余的预训练模型具体可参考\n",
    "\n",
    "https://link.zhihu.com/?target=https%3A//github.com/google-research/bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\n",
    "若想对Bert进行fine-tuning，如果存在out-of-memory的问题，可能会用到GPU并行：\n",
    "\n",
    "self.BertModel = nn.DataParallel(self.BertModel, device_ids=args.bert_gpu_ids, output_device=torch.cuda.current_device())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.下载预训练模型\n",
    "https://link.zhihu.com/?target=https%3A//github.com/ymcui/Chinese-BERT-wwm\n",
    "\n",
    "8.train.txt下载\n",
    "数据集选择的THUCNews，整理出18w条数据，10类新闻文本的中文分类问题（10分类），每类新闻数据量相等，为1.8w条，数据集来自\n",
    "https://github.com/649453932/Bert-Chinese-Text-Classification-Pytorch/tree/master/THUCNews/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from pytorch_pretrained_bert import BertModel, BertTokenizer, BertConfig, BertAdam\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tqdm import tqdm \n",
    "from torch.utils.data import *\n",
    "\n",
    "path = \"data/\"\n",
    "bert_path = \"chinese_roberta_wwm_ext_pytorch/\"\n",
    "tokenizer = BertTokenizer(vocab_file=bert_path + \"vocab.txt\")  # 初始化分词器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "782it [00:00, 7814.36it/s]\u001b[A\n",
      "1627it [00:00, 7992.13it/s]\u001b[A\n",
      "2513it [00:00, 8232.29it/s]\u001b[A\n",
      "3381it [00:00, 8361.50it/s]\u001b[A\n",
      "4248it [00:00, 8450.83it/s]\u001b[A\n",
      "5133it [00:00, 8565.06it/s]\u001b[A\n",
      "5978it [00:00, 8528.25it/s]\u001b[A\n",
      "6814it [00:00, 8468.77it/s]\u001b[A\n",
      "7629it [00:00, 8367.18it/s]\u001b[A\n",
      "8436it [00:01, 6955.41it/s]\u001b[A\n",
      "9252it [00:01, 7276.65it/s]\u001b[A\n",
      "10071it [00:01, 7526.08it/s]\u001b[A\n",
      "10946it [00:01, 7854.94it/s]\u001b[A\n",
      "11775it [00:01, 7977.18it/s]\u001b[A\n",
      "12583it [00:01, 7958.01it/s]\u001b[A\n",
      "13386it [00:01, 7975.48it/s]\u001b[A\n",
      "14189it [00:01, 7980.42it/s]\u001b[A\n",
      "15019it [00:01, 8072.46it/s]\u001b[A\n",
      "15867it [00:01, 8189.21it/s]\u001b[A\n",
      "16689it [00:02, 8042.92it/s]\u001b[A\n",
      "17514it [00:02, 8103.78it/s]\u001b[A\n",
      "18346it [00:02, 8166.68it/s]\u001b[A\n",
      "19179it [00:02, 8213.51it/s]\u001b[A\n",
      "20002it [00:02, 8201.43it/s]\u001b[A\n",
      "20823it [00:02, 8139.32it/s]\u001b[A\n",
      "21642it [00:02, 8153.06it/s]\u001b[A\n",
      "22472it [00:02, 8196.26it/s]\u001b[A\n",
      "23317it [00:02, 8270.67it/s]\u001b[A\n",
      "24160it [00:02, 8317.22it/s]\u001b[A\n",
      "24993it [00:03, 8178.05it/s]\u001b[A\n",
      "25812it [00:03, 8147.08it/s]\u001b[A\n",
      "26628it [00:03, 7980.91it/s]\u001b[A\n",
      "27457it [00:03, 8067.49it/s]\u001b[A\n",
      "28283it [00:03, 8122.33it/s]\u001b[A\n",
      "29097it [00:03, 8070.69it/s]\u001b[A\n",
      "29913it [00:03, 8095.95it/s]\u001b[A\n",
      "30724it [00:03, 8036.70it/s]\u001b[A\n",
      "31529it [00:03, 6300.49it/s]\u001b[A\n",
      "32355it [00:04, 6781.62it/s]\u001b[A\n",
      "33174it [00:04, 7149.97it/s]\u001b[A\n",
      "34056it [00:04, 7578.95it/s]\u001b[A\n",
      "34869it [00:04, 7735.35it/s]\u001b[A\n",
      "35669it [00:04, 7637.17it/s]\u001b[A\n",
      "36464it [00:04, 7728.24it/s]\u001b[A\n",
      "37287it [00:04, 7870.37it/s]\u001b[A\n",
      "38173it [00:04, 8141.81it/s]\u001b[A\n",
      "39001it [00:04, 8179.96it/s]\u001b[A\n",
      "39826it [00:04, 8154.26it/s]\u001b[A\n",
      "40647it [00:05, 7989.30it/s]\u001b[A\n",
      "41450it [00:05, 7986.76it/s]\u001b[A\n",
      "42252it [00:05, 7961.27it/s]\u001b[A\n",
      "43077it [00:05, 8045.36it/s]\u001b[A\n",
      "43900it [00:05, 8097.66it/s]\u001b[A\n",
      "44740it [00:05, 8185.84it/s]\u001b[A\n",
      "45575it [00:05, 8231.60it/s]\u001b[A\n",
      "46431it [00:05, 8326.73it/s]\u001b[A\n",
      "47275it [00:05, 8360.22it/s]\u001b[A\n",
      "48112it [00:06, 8302.33it/s]\u001b[A\n",
      "48943it [00:06, 8249.72it/s]\u001b[A\n",
      "49809it [00:06, 8368.08it/s]\u001b[A\n",
      "50656it [00:06, 8398.35it/s]\u001b[A\n",
      "51497it [00:06, 8381.47it/s]\u001b[A\n",
      "52336it [00:06, 8318.86it/s]\u001b[A\n",
      "53169it [00:06, 8256.67it/s]\u001b[A\n",
      "53996it [00:06, 5952.21it/s]\u001b[A\n",
      "54818it [00:06, 6489.14it/s]\u001b[A\n",
      "55649it [00:07, 6945.68it/s]\u001b[A\n",
      "56449it [00:07, 7229.87it/s]\u001b[A\n",
      "57220it [00:07, 7240.17it/s]\u001b[A\n",
      "57991it [00:07, 7343.75it/s]\u001b[A\n",
      "58787it [00:07, 7516.11it/s]\u001b[A\n",
      "59575it [00:07, 7619.31it/s]\u001b[A\n",
      "60375it [00:07, 7728.78it/s]\u001b[A\n",
      "61175it [00:07, 7806.55it/s]\u001b[A\n",
      "61991it [00:07, 7907.44it/s]\u001b[A\n",
      "62797it [00:07, 7950.41it/s]\u001b[A\n",
      "63596it [00:08, 7937.45it/s]\u001b[A\n",
      "64408it [00:08, 7990.69it/s]\u001b[A\n",
      "65235it [00:08, 8070.64it/s]\u001b[A\n",
      "66049it [00:08, 8087.25it/s]\u001b[A\n",
      "66859it [00:08, 7754.78it/s]\u001b[A\n",
      "67672it [00:08, 7860.05it/s]\u001b[A\n",
      "68477it [00:08, 7914.97it/s]\u001b[A\n",
      "69291it [00:08, 7981.06it/s]\u001b[A\n",
      "70091it [00:08, 7948.72it/s]\u001b[A\n",
      "70888it [00:08, 7909.33it/s]\u001b[A\n",
      "71680it [00:09, 7879.33it/s]\u001b[A\n",
      "72480it [00:09, 7914.19it/s]\u001b[A\n",
      "73272it [00:09, 7905.43it/s]\u001b[A\n",
      "74068it [00:09, 7920.32it/s]\u001b[A\n",
      "74861it [00:09, 7864.06it/s]\u001b[A\n",
      "75671it [00:09, 7931.08it/s]\u001b[A\n",
      "76513it [00:09, 8069.53it/s]\u001b[A\n",
      "77321it [00:09, 8065.25it/s]\u001b[A\n",
      "78129it [00:09, 8011.56it/s]\u001b[A\n",
      "78931it [00:09, 7954.38it/s]\u001b[A\n",
      "79727it [00:10, 5422.17it/s]\u001b[A\n",
      "80511it [00:10, 5974.95it/s]\u001b[A\n",
      "81238it [00:10, 6312.12it/s]\u001b[A\n",
      "81966it [00:10, 6573.06it/s]\u001b[A\n",
      "82749it [00:10, 6904.15it/s]\u001b[A\n",
      "83564it [00:10, 7235.12it/s]\u001b[A\n",
      "84428it [00:10, 7605.02it/s]\u001b[A\n",
      "85243it [00:10, 7759.27it/s]\u001b[A\n",
      "86040it [00:11, 7798.88it/s]\u001b[A\n",
      "86835it [00:11, 7666.39it/s]\u001b[A\n",
      "87633it [00:11, 7756.58it/s]\u001b[A\n",
      "88502it [00:11, 8013.21it/s]\u001b[A\n",
      "89359it [00:11, 8171.90it/s]\u001b[A\n",
      "90183it [00:11, 8117.71it/s]\u001b[A\n",
      "91000it [00:11, 8062.34it/s]\u001b[A\n",
      "91810it [00:11, 8012.83it/s]\u001b[A\n",
      "92617it [00:11, 8028.32it/s]\u001b[A\n",
      "93434it [00:11, 8069.30it/s]\u001b[A\n",
      "94243it [00:12, 8021.93it/s]\u001b[A\n",
      "95066it [00:12, 8078.17it/s]\u001b[A\n",
      "95875it [00:12, 8032.03it/s]\u001b[A\n",
      "96679it [00:12, 7982.54it/s]\u001b[A\n",
      "97502it [00:12, 8053.71it/s]\u001b[A\n",
      "98308it [00:12, 8000.92it/s]\u001b[A\n",
      "99158it [00:12, 8142.08it/s]\u001b[A\n",
      "99974it [00:12, 8142.60it/s]\u001b[A\n",
      "100789it [00:12, 8137.53it/s]\u001b[A\n",
      "101604it [00:12, 8088.31it/s]\u001b[A\n",
      "102414it [00:13, 8074.56it/s]\u001b[A\n",
      "103222it [00:13, 8020.47it/s]\u001b[A\n",
      "104051it [00:13, 8098.45it/s]\u001b[A\n",
      "104862it [00:13, 8037.05it/s]\u001b[A\n",
      "105683it [00:13, 8085.91it/s]\u001b[A\n",
      "106492it [00:13, 8080.26it/s]\u001b[A\n",
      "107307it [00:13, 8098.16it/s]\u001b[A\n",
      "108147it [00:13, 8179.76it/s]\u001b[A\n",
      "108966it [00:13, 8063.62it/s]\u001b[A\n",
      "109781it [00:13, 8088.62it/s]\u001b[A\n",
      "110591it [00:14, 4970.24it/s]\u001b[A\n",
      "111380it [00:14, 5590.87it/s]\u001b[A\n",
      "112211it [00:14, 6199.29it/s]\u001b[A\n",
      "113041it [00:14, 6707.00it/s]\u001b[A\n",
      "113880it [00:14, 7135.94it/s]\u001b[A\n",
      "114699it [00:14, 7419.72it/s]\u001b[A\n",
      "115508it [00:14, 7608.09it/s]\u001b[A\n",
      "116307it [00:14, 7593.38it/s]\u001b[A\n",
      "117099it [00:15, 7687.27it/s]\u001b[A\n",
      "117887it [00:15, 7730.14it/s]\u001b[A\n",
      "118683it [00:15, 7795.13it/s]\u001b[A\n",
      "119494it [00:15, 7886.92it/s]\u001b[A\n",
      "120290it [00:15, 7870.63it/s]\u001b[A\n",
      "121082it [00:15, 7859.58it/s]\u001b[A\n",
      "121913it [00:15, 7989.00it/s]\u001b[A\n",
      "122715it [00:15, 7975.23it/s]\u001b[A\n",
      "123517it [00:15, 7987.36it/s]\u001b[A\n",
      "124362it [00:16, 8120.28it/s]\u001b[A\n",
      "125176it [00:16, 8052.61it/s]\u001b[A\n",
      "126011it [00:16, 8137.81it/s]\u001b[A\n",
      "126826it [00:16, 8080.25it/s]\u001b[A\n",
      "127917it [00:16, 7780.15it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "input_ids = []     # input char ids\n",
    "input_types = []   # segment ids\n",
    "input_masks = []   # attention mask\n",
    "label = []         # 标签\n",
    "pad_size = 32      # 也称为 max_len (前期统计分析，文本长度最大值为38，取32即可覆盖99%)\n",
    " \n",
    "with open(path + \"train.txt\", encoding='utf-8') as f:\n",
    "    for i, l in tqdm(enumerate(f)): \n",
    "        try:\n",
    "            x1, y = l.strip().split('\\t')\n",
    "        except:\n",
    "            continue\n",
    "        x1 = tokenizer.tokenize(x1)\n",
    "        tokens = [\"[CLS]\"] + x1 + [\"[SEP]\"]\n",
    "        \n",
    "        # 得到input_id, seg_id, att_mask\n",
    "        ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        types = [0] *(len(ids))\n",
    "        masks = [1] * len(ids)\n",
    "        # 短则补齐，长则切断\n",
    "        if len(ids) < pad_size:\n",
    "            types = types + [1] * (pad_size - len(ids))  # mask部分 segment置为1\n",
    "            masks = masks + [0] * (pad_size - len(ids))\n",
    "            ids = ids + [0] * (pad_size - len(ids))\n",
    "        else:\n",
    "            types = types[:pad_size]\n",
    "            masks = masks[:pad_size]\n",
    "            ids = ids[:pad_size]\n",
    "        input_ids.append(ids)\n",
    "        input_types.append(types)\n",
    "        input_masks.append(masks)\n",
    "#         print(len(ids), len(masks), len(types)) \n",
    "        assert len(ids) == len(masks) == len(types) == pad_size\n",
    "        label.append([int(y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切分训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[105597, 73370, 23494, 7028, 106320, 72718, 13070, 103120, 48846, 98990]\n",
      "(102332, 32) (102332, 32) (102332, 32) (102332, 1)\n",
      "(25584, 32) (25584, 32) (25584, 32) (25584, 1)\n"
     ]
    }
   ],
   "source": [
    "# 随机打乱索引\n",
    "random_order = list(range(len(input_ids)))\n",
    "np.random.seed(2020)   # 固定种子\n",
    "np.random.shuffle(random_order)\n",
    "print(random_order[:10])\n",
    "\n",
    "# 4:1 划分训练集和测试集\n",
    "input_ids_train = np.array([input_ids[i] for i in random_order[:int(len(input_ids)*0.8)]])\n",
    "input_types_train = np.array([input_types[i] for i in random_order[:int(len(input_ids)*0.8)]])\n",
    "input_masks_train = np.array([input_masks[i] for i in random_order[:int(len(input_ids)*0.8)]])\n",
    "y_train = np.array([label[i] for i in random_order[:int(len(input_ids) * 0.8)]])\n",
    "print(input_ids_train.shape, input_types_train.shape, input_masks_train.shape, y_train.shape)\n",
    "\n",
    "input_ids_test = np.array([input_ids[i] for i in random_order[int(len(input_ids)*0.8):]])\n",
    "input_types_test = np.array([input_types[i] for i in random_order[int(len(input_ids)*0.8):]])\n",
    "input_masks_test = np.array([input_masks[i] for i in random_order[int(len(input_ids)*0.8):]])\n",
    "y_test = np.array([label[i] for i in random_order[int(len(input_ids) * 0.8):]])\n",
    "print(input_ids_test.shape, input_types_test.shape, input_masks_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载到高效的DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "train_data = TensorDataset(torch.LongTensor(input_ids_train), \n",
    "                           torch.LongTensor(input_types_train), \n",
    "                           torch.LongTensor(input_masks_train), \n",
    "                           torch.LongTensor(y_train))\n",
    "train_sampler = RandomSampler(train_data)  \n",
    "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_data = TensorDataset(torch.LongTensor(input_ids_test), \n",
    "                          torch.LongTensor(input_types_test), \n",
    "                         torch.LongTensor(input_masks_test),\n",
    "                          torch.LongTensor(y_test))\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义bert模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_path)  # /bert_pretrain/\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True  # 每个参数都要 求梯度\n",
    "        self.fc = nn.Linear(768, 10)   # 768 -> 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        context = x[0]  # 输入的句子   (ids, seq_len, mask)\n",
    "        types = x[1]\n",
    "        mask = x[2]  # 对padding部分进行mask，和句子相同size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]\n",
    "        _, pooled = self.bert(context, token_type_ids=types, \n",
    "                              attention_mask=mask, \n",
    "                              output_all_encoded_layers=False) # 控制是否输出所有encoder层的结果\n",
    "        out = self.fc(pooled)   # 得到10分类\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现，bert模型的定义由于高效简易的封装库存在，使得定义模型较为容易，如果想要在bert之后加入cnn/rnn等层，可在这里定义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化bert模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): BertLayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=768, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Model().to(DEVICE)\n",
    "print(model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())  # 模型参数名字列表\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "NUM_EPOCHS = 3\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=0.05,\n",
    "                     t_total=len(train_loader) * NUM_EPOCHS\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义训练函数和测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):   # 训练模型\n",
    "    model.train()\n",
    "    best_acc = 0.0 \n",
    "    for batch_idx, (x1,x2,x3, y) in enumerate(train_loader):\n",
    "        start_time = time.time()\n",
    "        x1,x2,x3, y = x1.to(device), x2.to(device), x3.to(device), y.to(device)\n",
    "        y_pred = model([x1, x2, x3])  # 得到预测结果\n",
    "        model.zero_grad()             # 梯度清零\n",
    "        loss = F.cross_entropy(y_pred, y.squeeze())  # 得到loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(batch_idx + 1) % 100 == 0:    # 打印loss\n",
    "            print('Train Epoch: {} [{}/{} ({:.2f}%)]\\tLoss: {:.6f}'.format(epoch, (batch_idx+1) * len(x1), \n",
    "                                                                           len(train_loader.dataset),\n",
    "                                                                           100. * batch_idx / len(train_loader), \n",
    "                                                                           loss.item()))  # 记得为loss.item()\n",
    "\n",
    "def test(model, device, test_loader):    # 测试模型, 得到测试集评估结果\n",
    "    model.eval()\n",
    "    test_loss = 0.0 \n",
    "    acc = 0 \n",
    "    for batch_idx, (x1,x2,x3, y) in enumerate(test_loader):\n",
    "        x1,x2,x3, y = x1.to(device), x2.to(device), x3.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_ = model([x1,x2,x3])\n",
    "        test_loss += F.cross_entropy(y_, y.squeeze())\n",
    "        pred = y_.max(-1, keepdim=True)[1]   # .max(): 2输出，分别为最大值和最大值的index\n",
    "        acc += pred.eq(y.view_as(pred)).sum().item()    # 记得加item()\n",
    "    test_loss /= len(test_loader)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "          test_loss, acc, len(test_loader.dataset),\n",
    "          100. * acc / len(test_loader.dataset)))\n",
    "    return acc / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0.0 \n",
    "PATH = 'roberta_model.pth'  # 定义模型保存路径\n",
    "for epoch in range(1, NUM_EPOCHS+1):  # 3个epoch\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    acc = test(model, DEVICE, test_loader)\n",
    "    if best_acc < acc: \n",
    "        best_acc = acc \n",
    "        torch.save(model.state_dict(), PATH)  # 保存最优模型\n",
    "    print(\"acc is: {:.4f}, best acc is {:.4f}\\n\".format(acc, best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载最优模型进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"roberta_model.pth\"))\n",
    "acc = test(model, DEVICE, test_loader)\n",
    "\n",
    "# 如果打比赛的话，下面代码也可参考\n",
    "\"\"\"\n",
    "# 测试集提交\n",
    "PATH = \"roberta_model.pth\"\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "def test_for_submit(model, device, test_loader):    # 测试模型\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for batch_idx, (x1,x2,x3) in tqdm(enumerate(test_loader)):\n",
    "        x1,x2,x3 = x1.to(device), x2.to(device), x3.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_ = model([x1,x2,x3])\n",
    "        pred = y_.max(-1, keepdim=True)[1].squeeze().cpu().tolist()   \n",
    "        # .max() 2输出，分别为最大值和最大值的index\n",
    "        preds.extend(pred) \n",
    "    return preds \n",
    "preds = test_for_submit(model, DEVICE, test_loader)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
