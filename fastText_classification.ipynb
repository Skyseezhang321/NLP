{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext.FastText as fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "部分标签分别为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper_tag = {\n",
    "    '财经': 'Finance',\n",
    "    '彩票': 'Lottery',\n",
    "    '房产': 'Property',\n",
    "    '股票': 'Shares',\n",
    "    '家居': 'Furnishing',\n",
    "    '教育': 'Education',\n",
    "    '科技': 'Technology',\n",
    "    '社会': 'Sociology',\n",
    "    '时尚': 'Fashion',\n",
    "    '时政': 'Affairs',\n",
    "    '体育': 'Sports',\n",
    "    '星座': 'Constellation',\n",
    "    '游戏': 'Game',\n",
    "    '娱乐': 'Entertainment'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理\n",
    "\n",
    "由于data.txt已经经过了分词和去停用词的处理，所以这里只需要对数据进行切割为训练集和测试集即可。\n",
    "\n",
    "分词和去停用词的工具代码(运行时不需要执行此部分代码)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/_4/prv7y_2n7h70w19g8bk7zxmr0000gn/T/jieba.cache\n",
      "Loading model cost 0.791 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from types import MethodType, FunctionType\n",
    "\n",
    "import jieba\n",
    "\n",
    "\n",
    "def clean_txt(raw):\n",
    "    fil = re.compile(r\"[^0-9a-zA-Z\\u4e00-\\u9fa5]+\")\n",
    "    return fil.sub(' ', raw)\n",
    "\n",
    "def seg(sentence, sw, apply=None):\n",
    "    if isinstance(apply, FunctionType) or isinstance(apply, MethodType):\n",
    "        sentence = apply(sentence)\n",
    "    return ' '.join([i for i in jieba.cut(sentence) if i.strip() and i not in sw])\n",
    "\n",
    "def stop_words():\n",
    "    with open('./data/stopwords.txt', 'r', encoding='utf-8') as swf:\n",
    "        return [line.strip() for line in swf]\n",
    "\n",
    "# 对某个sentence进行处理：\n",
    "content = '上海天然橡胶期价周三再创年内新高，主力合约突破21000元/吨重要关口。'\n",
    "res = seg(content.lower().replace('\\n', ''), stop_words(), apply=clean_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'上海 天然橡胶 期价 周三 再创 年内 新高 主力 合约 突破 21000 元 吨 关口'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切割数据 先将txt文件转换成csv文件，方便后面的计算 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class _MD(object):\n",
    "    mapper = {\n",
    "        str: '',\n",
    "        int: 0,\n",
    "        list: list,\n",
    "        dict: dict,\n",
    "        set: set,\n",
    "        bool: False,\n",
    "        float: .0\n",
    "    }\n",
    "\n",
    "    def __init__(self, obj, default=None):\n",
    "        self.dict = {}\n",
    "        assert obj in self.mapper, \\\n",
    "            'got a error type'\n",
    "        self.t = obj\n",
    "        if default is None:\n",
    "            return\n",
    "        assert isinstance(default, obj), \\\n",
    "            f'default ({default}) must be {obj}'\n",
    "        self.v = default\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.dict[key] = value\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if item not in self.dict and hasattr(self, 'v'):\n",
    "            self.dict[item] = self.v\n",
    "            return self.v\n",
    "        elif item not in self.dict:\n",
    "            if callable(self.mapper[self.t]):\n",
    "                self.dict[item] = self.mapper[self.t]()\n",
    "            else:\n",
    "                self.dict[item] = self.mapper[self.t]\n",
    "            return self.dict[item]\n",
    "        return self.dict[item]\n",
    "\n",
    "\n",
    "def defaultdict(obj, default=None):\n",
    "    return _MD(obj, default)\n",
    "\n",
    "\n",
    "class TransformData(object):\n",
    "    def to_csv(self, handler, output, index=False):\n",
    "        dd = defaultdict(list)\n",
    "        for line in handler:\n",
    "            label, content = line.split(',', 1)\n",
    "            dd[label.strip('__label__').strip()].append(content.strip())\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        for key in dd.dict:\n",
    "            col = pd.Series(dd[key], name=key)\n",
    "            df = pd.concat([df, col], axis=1)\n",
    "        return df.to_csv(output, index=index, encoding='utf-8')\n",
    "\n",
    "\n",
    "def split_train_test(source, auth_data=False):\n",
    "    if not auth_data:\n",
    "        train_proportion = 0.8\n",
    "    else:\n",
    "        train_proportion = 0.98\n",
    "\n",
    "    basename = source.rsplit('.', 1)[0]\n",
    "    train_file = basename + '_train.txt'\n",
    "    test_file = basename + '_test.txt'\n",
    "\n",
    "    handel = pd.read_csv(source, index_col=False, low_memory=False)\n",
    "    train_data_set = []\n",
    "    test_data_set = []\n",
    "    for head in list(handel.head()):\n",
    "        train_num = int(handel[head].dropna().__len__() * train_proportion)\n",
    "        sub_list = [f'__label__{head} , {item.strip()}\\n' for item in handel[head].dropna().tolist()]\n",
    "        train_data_set.extend(sub_list[:train_num])\n",
    "        test_data_set.extend(sub_list[train_num:])\n",
    "    shuffle(train_data_set)\n",
    "    shuffle(test_data_set)\n",
    "\n",
    "    with open(train_file, 'w', encoding='utf-8') as trainf,\\\n",
    "        open(test_file, 'w', encoding='utf-8') as testf:\n",
    "        for tds in train_data_set:\n",
    "            trainf.write(tds)\n",
    "        for i in test_data_set:\n",
    "            testf.write(i)\n",
    "\n",
    "    return train_file, test_file\n",
    "\n",
    "# 转化成csv\n",
    "td = TransformData()\n",
    "handler = open('./data/data.txt')\n",
    "td.to_csv(handler, './data/data.csv')\n",
    "handler.close()\n",
    "\n",
    "# 将csv文件切割，会生成两个文件（data_train.txt和data_test.txt）\n",
    "train_file, test_file = split_train_test('./data/data.csv', auth_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第四步：训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(989, 0.980788675429727, 0.980788675429727)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9885, 0.9740010116337886, 0.9740010116337886)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "def train_model(ipt=None, opt=None, model='', dim=100, epoch=5, lr=0.1, loss='softmax'):\n",
    "    np.set_printoptions(suppress=True)\n",
    "    if os.path.isfile(model):\n",
    "        classifier = fasttext.load_model(model)\n",
    "    else:\n",
    "        classifier = fasttext.train_supervised(ipt, label='__label__', dim=dim, epoch=epoch,\n",
    "                                         lr=lr, wordNgrams=2, loss=loss)\n",
    "        \"\"\"\n",
    "          训练一个监督模型, 返回一个模型对象\n",
    "\n",
    "          @param input:           训练数据文件路径\n",
    "          @param lr:              学习率\n",
    "          @param dim:             向量维度\n",
    "          @param ws:              cbow模型时使用\n",
    "          @param epoch:           次数\n",
    "          @param minCount:        词频阈值, 小于该值在初始化时会过滤掉\n",
    "          @param minCountLabel:   类别阈值，类别小于该值初始化时会过滤掉\n",
    "          @param minn:            构造subword时最小char个数\n",
    "          @param maxn:            构造subword时最大char个数\n",
    "          @param neg:             负采样\n",
    "          @param wordNgrams:      n-gram个数\n",
    "          @param loss:            损失函数类型, softmax, ns: 负采样, hs: 分层softmax\n",
    "          @param bucket:          词扩充大小, [A, B]: A语料中包含的词向量, B不在语料中的词向量\n",
    "          @param thread:          线程个数, 每个线程处理输入数据的一段, 0号线程负责loss输出\n",
    "          @param lrUpdateRate:    学习率更新\n",
    "          @param t:               负采样阈值\n",
    "          @param label:           类别前缀\n",
    "          @param verbose:         ??\n",
    "          @param pretrainedVectors: 预训练的词向量文件路径, 如果word出现在文件夹中初始化不再随机\n",
    "          @return model object\n",
    "        \"\"\"\n",
    "        classifier.save_model(opt)\n",
    "    return classifier\n",
    "\n",
    "dim = 100\n",
    "lr = 5\n",
    "epoch = 5\n",
    "model = f'data_dim{str(dim)}_lr0{str(lr)}_iter{str(epoch)}.model'\n",
    "\n",
    "classifier = train_model(ipt='./data/data_train.txt',\n",
    "                         opt=model,\n",
    "                         model=model,\n",
    "                         dim=dim, epoch=epoch, lr=0.5\n",
    "                         )\n",
    "\n",
    "result = classifier.test('./data/data_test.txt')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整体的结果为(测试数据量，precision，recall)：(9885, 0.9740010116337886, 0.9740010116337886)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出结果相当高，由于上面是将整体作为测试，fasttext只给出整体的结果，precision和recall是相同的，下面我们测试每个标签的precision、recall和F1值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_precision_and_recall(file='./data/data_test.txt'):\n",
    "    precision = defaultdict(int, 1)\n",
    "    recall = defaultdict(int, 1)\n",
    "    total = defaultdict(int, 1)\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            label, content = line.split(',', 1)\n",
    "            total[label.strip().strip('__label__')] += 1\n",
    "            labels2 = classifier.predict([seg(sentence=content.strip(), sw='', apply=clean_txt)])\n",
    "            pre_label, sim = labels2[0][0][0], labels2[1][0][0]\n",
    "            recall[pre_label.strip().strip('__label__')] += 1\n",
    "\n",
    "            if label.strip() == pre_label.strip():\n",
    "                precision[label.strip().strip('__label__')] += 1\n",
    "\n",
    "    print('precision', precision.dict)\n",
    "    print('recall', recall.dict)\n",
    "    print('total', total.dict)\n",
    "    for sub in precision.dict:\n",
    "        pre = precision[sub] / total[sub]\n",
    "        rec =  precision[sub] / recall[sub]\n",
    "        F1 = (2 * pre * rec) / (pre + rec)\n",
    "        print(f\"{sub.strip('__label__')}  precision: {str(pre)}  recall: {str(rec)}  F1: {str(F1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision {'Sports': 97, 'Sociology': 96, 'Education': 100, 'Shares': 100, 'Affairs': 101, 'Financ': 99, 'Entertainment': 101, 'Technology': 100, 'Furnishing': 100, 'Gam': 88}\n",
      "recall {'Sports': 98, 'Sociology': 99, 'Education': 102, 'Shares': 102, 'Affairs': 101, 'Financ': 100, 'Entertainment': 103, 'Technology': 100, 'Furnishing': 104, 'Gam': 90}\n",
      "total {'Sports': 101, 'Sociology': 101, 'Education': 101, 'Shares': 101, 'Affairs': 101, 'Financ': 101, 'Entertainment': 101, 'Technology': 101, 'Furnishing': 101, 'Gam': 89, 'Property': 2}\n",
      "Sports  precision: 0.9603960396039604  recall: 0.9897959183673469  F1: 0.9748743718592964\n",
      "Sociology  precision: 0.9504950495049505  recall: 0.9696969696969697  F1: 0.96\n",
      "Education  precision: 0.9900990099009901  recall: 0.9803921568627451  F1: 0.9852216748768472\n",
      "Shares  precision: 0.9900990099009901  recall: 0.9803921568627451  F1: 0.9852216748768472\n",
      "Affairs  precision: 1.0  recall: 1.0  F1: 1.0\n",
      "Financ  precision: 0.9801980198019802  recall: 0.99  F1: 0.9850746268656716\n",
      "Entertainment  precision: 1.0  recall: 0.9805825242718447  F1: 0.9901960784313726\n",
      "Technology  precision: 0.9900990099009901  recall: 1.0  F1: 0.9950248756218906\n",
      "Furnishing  precision: 0.9900990099009901  recall: 0.9615384615384616  F1: 0.975609756097561\n",
      "Gam  precision: 0.9887640449438202  recall: 0.9777777777777777  F1: 0.9832402234636872\n"
     ]
    }
   ],
   "source": [
    "cal_precision_and_recall(file='./data/data_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(source):\n",
    "    basename = source.rsplit('.', 1)[0]\n",
    "    csv_file = basename + '.csv'\n",
    "\n",
    "    td = TransformData()\n",
    "    handler = open(source)\n",
    "    td.to_csv(handler, csv_file)\n",
    "    handler.close()\n",
    "\n",
    "    train_file, test_file = split_train_test(csv_file)\n",
    "\n",
    "    dim = 100\n",
    "    lr = 5\n",
    "    epoch = 5\n",
    "    model = f'data/data_dim{str(dim)}_lr0{str(lr)}_iter{str(epoch)}.model'\n",
    "\n",
    "    classifier = train_model(ipt=train_file,\n",
    "                             opt=model,\n",
    "                             model=model,\n",
    "                             dim=dim, epoch=epoch, lr=0.5\n",
    "                             )\n",
    "\n",
    "    result = classifier.test(test_file)\n",
    "    print(result)\n",
    "\n",
    "    cal_precision_and_recall(test_file)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main('data.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
